# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOtbwzoQU3_5LjriS34RGCKfO9r-bbwJ
"""

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
!pip install pillow numpy
!pip install git+https://github.com/facebookresearch/segment-anything.git



from google.colab import files
uploaded = files.upload()













import shutil
shutil.move("Screenshot 2025-09-17 171838.png", "data/site_images/room1.png")

from google.colab import files
uploaded = files.upload()

import shutil
shutil.move("Screenshot 2025-09-16 162933.png", "data/floor_textures/textures.png")

!mkdir -p sam
!wget -O sam/sam_model.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth

import torch
import numpy as np
from PIL import Image
from segment_anything import sam_model_registry, SamPredictor

# Device select
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load SAM model
sam = sam_model_registry["vit_b"](checkpoint="sam/sam_model.pth").to(device)
predictor = SamPredictor(sam)

def generate_floor_mask(image_path, save_path="floor_mask.png"):
    """
    Input: room image path
    Output: floor mask saved as PNG
    """
    image = Image.open(image_path).convert("RGB")
    image_np = np.array(image)
    predictor.set_image(image_np)

    # Predict mask (full image as placeholder)
    mask = predictor.predict(point_coords=None, point_labels=None)[0]
    print(f"Mask shape before squeeze: {mask.shape}")
    # Select the first mask if there are multiple
    if mask.ndim == 3 and mask.shape[0] > 1:
        mask = mask[0]
        print(f"Mask shape after selecting first mask: {mask.shape}")
    mask = np.squeeze(mask)
    print(f"Mask shape after squeeze: {mask.shape}")
    mask_uint8 = (mask > 0).astype(np.uint8) * 255
    mask_img = Image.fromarray(mask_uint8)
    mask_img.save(save_path)
    print(f"[SAM] Floor mask saved at {save_path}")
    return save_path


# Test run
if __name__ == "__main__":
    # Step 1: Generate mask from SAM
    mask_path = generate_floor_mask("data/site_images/room1.png")



import cv2
import numpy as np

# Load room and texture
room = cv2.imread("data/site_images/room1.png")
texture = cv2.imread("data/floor_textures/textures.png")

# Load SAM mask (binary floor mask: 255 = floor, 0 = non-floor)
mask = cv2.imread("floor_mask.png", 0)
mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

# Get size of room image
room_h, room_w, _ = room.shape

# --- Step 1: Tile texture to cover the whole room ---
tex_h, tex_w, _ = texture.shape
rep_x = int(np.ceil(room_w / tex_w))
rep_y = int(np.ceil(room_h / tex_h))

# Tile texture
tiled_texture = np.tile(texture, (rep_y, rep_x, 1))
tiled_texture = tiled_texture[:room_h, :room_w]  # crop to room size

# --- Step 2: Mask texture to floor shape ---
mask_3c = cv2.merge([mask, mask, mask])  # make mask 3-channel
floor_with_texture = cv2.bitwise_and(tiled_texture, mask_3c)

# --- Step 3: Remove old floor and combine ---
room_without_floor = cv2.bitwise_and(room, cv2.bitwise_not(mask_3c))
combined = cv2.add(room_without_floor, floor_with_texture)

# --- Step 4: Optional Poisson blending for realism ---
mask_sc = mask.astype(np.uint8)
if mask_sc.shape != (room_h, room_w):
    mask_sc = cv2.resize(mask_sc, (room_w, room_h))

# Shrink mask a bit to avoid ROI issues
kernel = np.ones((5, 5), np.uint8)
mask_safe = cv2.erode(mask_sc, kernel, iterations=2)

# Find safe center inside floor
contours, _ = cv2.findContours(mask_safe, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if contours:
    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
    center = (x + w//2, y + h//2)
else:
    center = (room_w//2, room_h//2)

print("Safe center:", center)

blended = cv2.seamlessClone(floor_with_texture, room, mask_safe, center, cv2.MIXED_CLONE)
cv2.imwrite("tiled_texture.png", tiled_texture)
cv2.imwrite("combined_floor.png", combined)
cv2.imwrite("blended_floor.png", blended)

print("âœ… Done! Results saved as combined_floor.png and blended_floor.png")

!pip install diffusers["torch"] transformers
!pip install accelerate
!pip install git+https://github.com/huggingface/diffusers

import cv2
import numpy as np
import os
import ipywidgets as widgets
from IPython.display import display
from google.colab.patches import cv2_imshow


def make_pattern(texture, size, pattern="square_grid"):
    h, w = size
    tex_h, tex_w = texture.shape[:2]
    canvas = np.zeros((h, w, 3), dtype=np.uint8)

    if pattern == "square_grid":
        # simple tiling
        for y in range(0, h, tex_h):
            for x in range(0, w, tex_w):
                canvas[y:y+tex_h, x:x+tex_w] = texture[:min(tex_h,h-y), :min(tex_w,w-x)]

    elif pattern == "herringbone":
        # alternate 90-degree rotated
        for i, y in enumerate(range(0, h, tex_h)):
            for j, x in enumerate(range(0, w, tex_w)):
                if (i+j) % 2 == 0:
                    tile = texture
                else:
                    tile = cv2.rotate(texture, cv2.ROTATE_90_CLOCKWISE)
                canvas[y:y+tex_h, x:x+tex_w] = tile[:min(tex_h,h-y), :min(tex_w,w-x)]

    elif pattern == "diagonal":
        M = cv2.getRotationMatrix2D((tex_w//2, tex_h//2), 45, 1.0)
        rotated = cv2.warpAffine(texture, M, (tex_w, tex_h))
        for y in range(0, h, tex_h):
            for x in range(0, w, tex_w):
                canvas[y:y+tex_h, x:x+tex_w] = rotated[:min(tex_h,h-y), :min(tex_w,w-x)]

    elif pattern == "brick":
        for i, y in enumerate(range(0, h, tex_h)):
            for j, x in enumerate(range(0, w, tex_w)):
                x_offset = (tex_w//2) if i%2==1 else 0
                x_pos = (x+x_offset) % w
                canvas[y:y+tex_h, x_pos:x_pos+tex_w] = texture[:min(tex_h,h-y), :min(tex_w,w-x_pos)]

    elif pattern == "basketweave":
        for i, y in enumerate(range(0, h, tex_h)):
            for j, x in enumerate(range(0, w, tex_w)):
                if (i+j)%2==0:
                    tile = cv2.resize(texture,(tex_w,tex_h))
                else:
                    tile = cv2.rotate(texture, cv2.ROTATE_90_CLOCKWISE)
                canvas[y:y+tex_h, x:x+tex_w] = tile[:min(tex_h,h-y), :min(tex_w,w-x)]

    elif pattern == "chevron":
        flipped = cv2.flip(texture,1)
        for i, y in enumerate(range(0,h,tex_h)):
            for j, x in enumerate(range(0,w,tex_w)):
                tile = texture if (j%2==0) else flipped
                canvas[y:y+tex_h, x:x+tex_w] = tile[:min(tex_h,h-y), :min(tex_w,w-x)]

    elif pattern == "hexagon":
        # approximate honeycomb tiling
        step_x, step_y = tex_w, int(tex_h*0.87)
        for i,y in enumerate(range(0,h,step_y)):
            for j,x in enumerate(range(0,w,step_x)):
                x_offset = (tex_w//2) if i%2==1 else 0
                x_pos = (x+x_offset) % w
                canvas[y:y+tex_h, x_pos:x_pos+tex_w] = texture[:min(tex_h,h-y), :min(tex_w,w-x_pos)]

    elif pattern == "subway":
        # simple rectangular tiles staggered
        half_w = tex_w//2
        tile = cv2.resize(texture,(half_w,tex_h))
        for i,y in enumerate(range(0,h,tex_h)):
            for j,x in enumerate(range(0,w,half_w)):
                x_offset = half_w//2 if i%2==1 else 0
                x_pos = (x+x_offset)%w
                canvas[y:y+tex_h, x_pos:x_pos+half_w] = tile[:min(tex_h,h-y), :min(half_w,w-x_pos)]

    elif pattern == "versailles":
        # alternating blocks (simple approximation)
        big = cv2.resize(texture,(tex_w*2,tex_h*2))
        for y in range(0,h,2*tex_h):
            for x in range(0,w,2*tex_w):
                canvas[y:y+2*tex_h, x:x+2*tex_w] = big[:min(2*tex_h,h-y), :min(2*tex_w,w-x)]

    elif pattern == "random_stone":
        for _ in range(200):  # random patches
            x = np.random.randint(0,w-tex_w)
            y = np.random.randint(0,h-tex_h)
            canvas[y:y+tex_h, x:x+tex_w] = texture
    return canvas


# --------------------------
# Load fixed inputs
# --------------------------
room = cv2.imread("data/site_images/room1.png")
mask = cv2.imread("floor_mask.png",0)
texture = cv2.imread("data/floor_textures/textures.png")
os.makedirs("objects",exist_ok=True)

# --------------------------
# Dropdown with all patterns
# --------------------------
patterns = [
    "square_grid","herringbone","diagonal","brick",
    "basketweave","chevron","hexagon","subway",
    "versailles","random_stone"
]

pattern_selector = widgets.Dropdown(
    options=patterns,
    description="Pattern:",
    value="square_grid"
)
display(pattern_selector)

selected_pattern = {"name": None, "path": None}

def update_pattern(change):
    pattern_type = pattern_selector.value
    pattern_img = make_pattern(texture,(room.shape[0],room.shape[1]),pattern=pattern_type)

    print(f"selected pattern: {pattern_type}")

    final = room.copy()
    final[mask > 128] = pattern_img[mask > 128]

    save_path = f"objects/floor_{pattern_type}.png"
    cv2.imwrite(save_path, final)

    # update global dict
    selected_pattern["name"] = pattern_type
    selected_pattern["path"] = save_path

    print(f"âœ… Applied: {pattern_type}")
    print(f"ðŸ“‚ Saved at: {save_path}")
    cv2_imshow(final)

# attach observer
pattern_selector.observe(update_pattern, names="value")

# Default run
update_pattern(None)

import os
from torch import manual_seed
import torch
from PIL import Image
from diffusers import StableDiffusionInpaintPipeline

# 1. Load Model
device = "cpu"
model_id_or_path = "runwayml/stable-diffusion-inpainting"  # inpainting model
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    model_id_or_path,
    torch_dtype=torch.float32
).to(device)

# 2. Input Images
init_image = Image.open( selected_pattern["path"]).convert("RGB").resize((768, 512))
mask_image = Image.open("floor_mask.png").convert("L").resize((768, 512))
# mask_image: white = refine area (floor), black = keep as is

prompt = (
    "Ultra-realistic refined floor texture in {selected_pattern['name']} pattern and floor texture with natural lighting, sharp details, "
    "perfectly blended with the room, photorealistic, professional interior look"
)

negative_prompt = (
    "low quality, blurry, distorted, unrealistic, extra objects, warped patterns, "
    "wrong perspective, artifacts, overexposed, underexposed,Â noisy"
)


# 1. Output folder
output_dir = "floor_variations"
os.makedirs(output_dir, exist_ok=True)

# 2. Generate multiple variations
results = []
for i in range(4):   # jitne outputs chahiye
    generator = torch.Generator("cpu").manual_seed(1234 + i)  # alag seed for diversity
    out = pipe(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=init_image,
        mask_image=mask_image,
        strength=0.7,
        guidance_scale=7.5,
        num_inference_steps=40,
        generator=generator
    ).images[0]

    file_path = os.path.join(output_dir, f"refined_floor_seed_{i+1}.png")
    out.save(file_path)
    results.append(out)

print(f"âœ… {len(results)} variations saved inside folder: {output_dir}")

