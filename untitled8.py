# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSCTB-U_OpvGhQOxyCZQWeo89pDLWKl4
"""





!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
!pip install pillow numpy
!pip install git+https://github.com/facebookresearch/segment-anything.git

from google.colab import files
uploaded = files.upload()

import shutil
shutil.move("room1.png", "data/site_images/room1.png")

!mkdir -p sam
!wget -O sam/sam_model.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth

import torch
import numpy as np
from PIL import Image
from segment_anything import sam_model_registry, SamPredictor

# Device select
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load SAM model
sam = sam_model_registry["vit_b"](checkpoint="sam/sam_model.pth").to(device)
predictor = SamPredictor(sam)

def generate_floor_mask(image_path, save_path="floor_mask.png"):
    """
    Input: room image path
    Output: floor mask saved as PNG
    """
    image = Image.open(image_path).convert("RGB")
    image_np = np.array(image)
    predictor.set_image(image_np)

    # Predict mask (full image as placeholder)
    mask = predictor.predict(point_coords=None, point_labels=None)[0]
    print(f"Mask shape before squeeze: {mask.shape}")
    # Select the first mask if there are multiple
    if mask.ndim == 3 and mask.shape[0] > 1:
        mask = mask[0]
        print(f"Mask shape after selecting first mask: {mask.shape}")
    mask = np.squeeze(mask)
    print(f"Mask shape after squeeze: {mask.shape}")
    mask_uint8 = (mask > 0).astype(np.uint8) * 255
    mask_img = Image.fromarray(mask_uint8)
    mask_img.save(save_path)
    print(f"[SAM] Floor mask saved at {save_path}")
    return save_path


# Test run
if __name__ == "__main__":
    # Step 1: Generate mask from SAM
    mask_path = generate_floor_mask("room1.png")

!pip install modelscope
!pip install safetensors

!pip install huggingface_hub safetensors

from huggingface_hub import hf_hub_download

# Example: Download a safetensors file from Nexus-Gen
file_path = hf_hub_download(
    repo_id="modelscope/Nexus-Gen",
    filename="model-00001-of-00004.safetensors",
    cache_dir="./nexus_gen"
)
print("Downloaded to:", file_path)

!pip install huggingface_hub safetensors

from huggingface_hub import hf_hub_download

# Example: Download a safetensors file from Nexus-Gen
file_path = hf_hub_download(
    repo_id="modelscope/Nexus-Gen",
    filename="model-00002-of-00004.safetensors",
    cache_dir="./nexus_gen"
)
print("Downloaded to:", file_path)

!pip install huggingface_hub safetensors

from huggingface_hub import hf_hub_download

# Example: Download a safetensors file from Nexus-Gen
file_path = hf_hub_download(
    repo_id="modelscope/Nexus-Gen",
    filename="model-00003-of-00004.safetensors",
    cache_dir="./nexus_gen"
)
print("Downloaded to:", file_path)

!pip install huggingface_hub safetensors

from huggingface_hub import hf_hub_download

# Example: Download a safetensors file from Nexus-Gen
file_path = hf_hub_download(
    repo_id="modelscope/Nexus-Gen",
    filename="model-00004-of-00004.safetensors",
    cache_dir="./nexus_gen"
)
print("Downloaded to:", file_path)

from google.colab import files
uploaded = files.upload()

import shutil
shutil.move("Screenshot 2025-09-16 162933.png", "Screenshot 2025-09-16 162933.png")

!pip uninstall -y datasets
!pip install datasets==2.14.6
!pip install modelscope==1.9.5 safetensors addict

!pip install diffusers==0.24.0 transformers==4.36.2 huggingface_hub==0.19.4 accelerate==0.25.0 safetensors xformers

import torch
from diffusers import StableDiffusionInpaintPipeline
from PIL import Image

# Load input (rough floor image)
input_path = "rough_floor.png"   # <-- yahi use karega
mask_path = "floor_mask.png"     # white = replace, black = keep

image = Image.open(input_path).convert("RGB")
mask_image = Image.open(mask_path).convert("L")

# Load open-source SD 2.0 inpainting
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16
).to("cuda")

prompt = "realistic carpet flooring, sharp details, professional interior photo"
negative_prompt = "blurry, distorted, low quality"

output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=image,
    mask_image=mask_image,
    strength=0.85,
    guidance_scale=7.5,
    num_inference_steps=40
)

result = output.images[0]
result.save("enhanced_carpet.png")
result.show()

from PIL import Image

def apply_texture(site_img, mask_img, texture_img, out_path="output_texture.png"):
    site = Image.open(site_img).convert("RGB").resize((512,512))
    mask = Image.open(mask_img).convert("L").resize((512,512))
    texture = Image.open(texture_img).convert("RGB").resize((512,512))

    # Paste texture on masked region
    site.paste(texture, (0,0), mask)
    site.save(out_path)
    return site

# Run with your actual file paths
rough_out = apply_texture(
    "data/site_images/room1.png",                       # site image
    "floor_mask.png",                                   # mask
    "data/floor_textures/Screenshot 2025-09-16 162933.png",  # texture with full path
    "rough_floor.png"                                   # output file
)

rough_out.show()
print("✅ Rough flooring applied (saved as rough_floor.png)")

!pip uninstall -y peft accelerate diffusers transformers huggingface_hub
!pip install --upgrade pip
!pip install accelerate==0.27.2 diffusers==0.27.2 transformers==4.41.2 huggingface_hub==0.23.2 safetensors==0.4.2

import torch
from diffusers import StableDiffusionInpaintPipeline
print("✅ diffusers loaded OK")

!pip install -q diffusers transformers accelerate safetensors opencv-python pillow

import torch
from diffusers import StableDiffusionInpaintPipeline
from PIL import Image

# Load input (rough floor image)
input_path = "rough_floor.png"   # <-- yahi use karega
mask_path = "floor_mask.png"     # white = replace, black = keep

image = Image.open(input_path).convert("RGB")
mask_image = Image.open(mask_path).convert("L")

# Load open-source SD 2.0 inpainting
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16
).to("cuda")

prompt = "realistic carpet flooring, sharp details, professional interior photo"
negative_prompt = "blurry, distorted, low quality"

output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=image,
    mask_image=mask_image,
    strength=0.85,
    guidance_scale=7.5,
    num_inference_steps=40
)

result = output.images[0]
result.save("enhanced_carpet.png")
result.show()

import cv2
from PIL import Image

# Input file
img_path = "rough_floor.png"

# Read image
image = cv2.imread(img_path)

# Sharpening kernel
kernel = np.array([[0, -1, 0],
                   [-1, 5,-1],
                   [0, -1, 0]])

# Apply filter
sharpened = cv2.filter2D(image, -1, kernel)

# Save and show
cv2.imwrite("sharp_floor.png", sharpened)

# Show output
Image.open("sharp_floor.png").show()

!python --version

3

